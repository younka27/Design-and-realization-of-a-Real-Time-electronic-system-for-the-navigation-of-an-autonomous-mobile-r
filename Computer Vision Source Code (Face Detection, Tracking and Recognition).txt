############For face detection and tracking############

from picamera.array import PiRGBArray
from picamera import PiCamera
import time
import cv2
import os

### Installation ###
# Center coordinates
cx = 160
cy = 120

os.system( "echo 0=150 > /dev/servoblaster" )
os.system( "echo 1=150 > /dev/servoblaster" )

xdeg = 150
ydeg = 150

# Setup the camera
camera = PiCamera()
camera.resolution = ( 320, 240 )
camera.framerate = 60
rawCapture = PiRGBArray( camera, size=( 320, 240 ) )

# Load a cascade file for detecting faces
face_cascade = cv2.CascadeClassifier( '/home/pi/opencv-2.4.9/data/lbpcascades/lbpcascade_frontalface.xml' ) 
t_start = time.time()
fps = 0

### Main ###

# Capture camera frames
for frame in camera.capture_continuous( rawCapture, format="bgr", use_video_port=True ):
    
    image = frame.array
   
# Use the waterfall file we loaded to detect faces
    gray = cv2.cvtColor( image, cv2.COLOR_BGR2GRAY )
    faces = face_cascade.detectMultiScale( gray )
    
    print "Found " + str( len( faces ) ) + " face(s)"
    
# Draw a rectangle around each face and move the motor to the face    
for ( x, y, w, h ) in faces:
        cv2.rectangle( image, ( x, y ), ( x + w, y + h ), ( 100, 255, 100 ), 2 

 cv2.putText( image, "Face No." + str( len( faces ) ), ( x, y ), 
cv2.FONT_HERSHEY_SIMPLEX, 0.5, ( 0, 0, 255 ), 2 )
        tx = x + w/2
        ty = y + h/2

        if   ( cx - tx >  10 and xdeg <= 190 ):
            xdeg += 3
            os.system( "echo 0=" + str( xdeg ) + " > /dev/servoblaster" )
        elif ( cx - tx < -10 and xdeg >= 110 ):
            xdeg -= 3
            os.system( "echo 0=" + str( xdeg ) + " > /dev/servoblaster" )
        
        if   ( cy - ty >  10 and ydeg >= 110 ):
            ydeg -= 3
            os.system( "echo 1=" + str( ydeg ) + " > /dev/servoblaster" )
        elif ( cy - ty < -10 and ydeg <= 190 ):
            ydeg += 3
            os.system( "echo 1=" + str( ydeg ) + " > /dev/servoblaster" )

    # Calculez et montrez le FPS
    fps = fps + 1
    sfps = fps / ( time.time() - t_start )
    cv2.putText( image, "FPS : " + str( int( sfps ) ), ( 10, 10 ), cv2.FONT_HERSHEY_SIMPLEX, 0.5, ( 0, 0, 255 ), 2 )    
    
    # Show frame 
    cv2.imshow( "Frame", image )
    cv2.waitKey( 1 )

    # Clear stream in preparation for next raw Capture frame.truncate ( 0 )


########## For Facial Recognition ############

#Program to capture positive images

import glob
import os
import sys
import select
import cv2
import config
import face

# Set the names of the learning images
POSITIVE_FILE_PREFIX = 'name_'
def is_letter_input(letter):

# Utility function to check if a specific character is available on stdin.
	if select.select([sys.stdin,],[],[],0.0)[0]:
		input_char = sys.stdin.read(1)
		return input_char.lower() == letter.lower()
	return False
if __name__ == '__main__':
	camera = config.get_camera()

# Create the directory for the positive training images if it does not exist
	if not os.path.exists(config.POSITIVE_DIR):
		os.makedirs(config.POSITIVE_DIR)
	# Find the largest id of existing positive images
	# Start new images after this ID value
	files = sorted(glob.glob(os.path.join(config.POSITIVE_DIR, 
		POSITIVE_FILE_PREFIX + '[0-9][0-9][0-9].pgm')))
	count = 0
if len(files) > 0:
		# Take count from last filename
		count = int(files[-1][-7:-4])+1
	print 'Capturing positive training images.'
	print 'Press button or type c (and press enter) to capture an image.'
	print 'Press Ctrl-C to quit.'
	while True:
# Check if the button was pressed or 'c' was received, then capture the image.
if is_letter_input('c'):
print 'Capturing image...'

image = camera.read()
	# Convert the image to grayscale
	image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
	# Get unique face coordinates in captured image
	result = face.detect_single(image)
	if result is None:
print 'Could not detect single face!  Check the image in capture.pgm' \  ' to see what was captured and try again with only one face visible.'
      continue
x, y, w, h = result
# Crop image as close as possible to desired face aspect ratio.
# Might be smaller if face is near edge of image.
crop = face.crop(image, x, y, w, h)
# Save image to file.
filename = os.path.join(config.POSITIVE_DIR, POSITIVE_FILE_PREFIX + '%03d.pgm' % count)
cv2.imwrite(filename, crop)
print 'Found face and wrote training image', filename
count += 1

#Program for training

import fnmatch
import os
import cv2
import numpy as np
import config
import face
MEAN_FILE = 'mean.png'
POSITIVE_EIGENFACE_FILE = 'positive_eigenface.png'

NEGATIVE_EIGENFACE_FILE = 'negative_eigenface.png'
def walk_files(directory, match='*'):
	for root, dirs, files in os.walk(directory):
		for filename in fnmatch.filter(files, match):
			yield os.path.join(root, filename)

def prepare_image(filename):

return 
face.resize(cv2.imread(filename, cv2.IMREAD_GRAYSCALE))

def normalize(X, low, high, dtype=None):
	
	X = np.asarray(X)
	minX, maxX = np.min(X), 
               np.max(X)
	# normalize to [0...1].
	X = X - float(minX)
	X = X / float((maxX - minX))
	# scale to [low...high].
	X = X * (high-low)
	X = X + low
	if dtype is None:
		return np.asarray(X)
	return np.asarray(X, dtype=dtype)

if __name__ == '__main__':
	print "Reading training images..."
	faces = []
	labels = []
	pos_count = 0
	neg_count = 0
	# Read all positive images
	for filename in walk_files(config.POSITIVE_DIR, '*.pgm'):
                path = os.path.basename(filename)
                path = path.split("_")[0]
                print path
                if(path == "name"):
                        faces.append(prepare_image(filename))
                        labels.append(config.name_LABEL) pos_count += 1
# Read all negative images
for filename in walk_files(config.NEGATIVE_DIR, '*.pgm'):
faces.append(prepare_image(filename))
abels.append(config.NEGATIVE_LABEL)
neg_count += 1
	print 'Read', pos_count, 'positive images and', neg_count, 'negative images.'

	# Training the model
	print 'Training model...'
	model = cv2.createEigenFaceRecognizer()
	model.train(np.asarray(faces), np.asarray(labels))

# save the model 
model.save(config.TRAINING_FILE)
	print 'Training data saved to', config.TRAINING_FILE

mean=model.getMat("mean").reshape(faces[0].shape)
	cv2.imwrite(MEAN_FILE, normalize(mean, 0, 255, dtype=np.uint8))
eigenvectors =model.getMat("eigenvectors")
pos_eigenvector=eigenvectors[:,0].reshape(faces[0].shape)
	cv2.imwrite(POSITIVE_EIGENFACE_FILE, normalize(pos_eigenvector, 0, 255, dtype=np.uint8))
neg_eigenvector=eigenvectors[:,1].reshape(faces[0].shape)
	cv2.imwrite(NEGATIVE_EIGENFACE_FILE, normalize(neg_eigenvector, 0, 255
 

# test facial recognition

# import the necessary packages
from imutils.video import VideoStream
import datetime
import argparse
import imutils
import time

import cv2
from opencv import config
from opencv import face

# build the analytical argument 
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--picamera", type=int, default=1,

        help="whether or not the Raspberry Pi camera should be used")
args = vars(ap.parse_args())

# Initialize the video stream and allow the camera sensor to warm up
vs = VideoStream(usePiCamera=args["picamera"] > 0).start()
time.sleep(2.0)

faceCascade = cv2.CascadeClassifier("opencv/haarcascade_frontalface_default.xml")
print 'Loading training data...'

model = cv2.createEigenFaceRecognizer()
model.load("opencv/training.xml")

print 'Training data loaded!'

while True:
# Take the frame from the video stream and resize it
# Have a maximum width of 400 pixels
        frame = vs.read()
        frame = imutils.resize(frame, width=400)

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        faces = faceCascade.detectMultiScale(
                gray,
                scaleFactor=1.3,
                minNeighbors=5,
                minSize=(30, 30),
                flags=cv2.cv.CV_HAAR_SCALE_IMAGE
        )

        # Draw a rectangle around the faces
        for (x, y, w, h) in faces:
                # Crop and resize face image
                crop = face.resize(face.crop(gray, x, y, w, h))
                # Test the face against the model
                label, confidence = model.predict(crop)
                
                #print 'Predicted {0} face with confidence {1} (lower is more confident).'.format(
                #        'POSITIVE' if label == config.POSITIVE_LABEL else 'NEGATIVE',
                #        confidence)
print 'Predicted {0} face with confidence {1} (lower is more confident).'.format(label, confidence)
                if label == config.name_LABEL and confidence < 3000:
                        #print 'Recognized face!'
                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                        
                else:
                        #print 'Did not recognize face!'
                        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)                

        # show frame
        cv2.imshow("Frame", frame)
        key = cv2.waitKey(1) & 0xFF

        # if the `q` key was pressed, break from the loop
        if key == ord("q"):
                break

cv2.destroyAllWindows()
vs.stop()






